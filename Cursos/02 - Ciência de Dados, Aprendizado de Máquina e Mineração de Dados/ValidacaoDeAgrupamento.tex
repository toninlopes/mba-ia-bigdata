\chapter{Validação de Agrupamento}

A validação de agrupamento é uma etapa crítica em aprendizado não supervisionado. O objetivo é determinar o quão "bom" é um agrupamento, baseando-se em dois critérios fundamentais:

\begin{itemize}
	\item \textbf{Coesão (ou Compactação)}: Mede o quão próximos e similares são os objetos dentro do mesmo cluster. Um bom cluster é altamente coeso.
	
	\item \textbf{Separação}: Mede o quão distintos e distantes são os objetos de diferentes clusters. Bons agrupamentos têm alta separação entre seus clusters.
\end{itemize}

Como não há uma "resposta correta" pré-definida, utilizamos métricas que avaliam a estrutura interna dos dados. Essas métrias se dividem em três categorias, conforme ilustrado na figura \ref{fig:indice-validacao}) abaixo.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\linewidth]{"Cursos/02 - Ciência de Dados, Aprendizado de Máquina e Mineração de Dados/images/Indice-Validacao.png"}
	\caption{Principais Índices}
	\label{fig:indice-validacao}
\end{figure}


\section{Para que é Utilizado?}

A validação é utilizada para responder a três perguntas cruciais:

\begin{enumerate}
	\item \textbf{Determinar o número ideal de clusters ($k$)}: Esta é a aplicação mais comum. A validação nos ajuda a encontrar o valor de $k$ que melhor representa a estrutura natural dos dados.
	
	\item \textbf{Comparar diferentes algoritmos ou configurações}: Qual algoritmo (K-Means, Hierárquico) ou qual configuração (ex: tipo de linkage no Hierárquico) funciona melhor para os meus dados?	
	
	\item \textbf{Avaliar a qualidade e o significado}: O agrupamento encontrado é estatisticamente significativo ou é apenas um resultado aleatório?
\end{enumerate}

\section{Inspeção Visual}

Consiste em visualizar a matriz de dissimilaridades (ou similaridades) dos dados, mas ordenando os objetos de acordo com os clusters encontrados pelo algoritmo.

É uma ferramenta de exploração inicial rápida. Em uma matriz termo-documento, por exemplo, os clusters bem formados aparecem como "blocos" ou "quadrados" algongados na diagonal principal, indicando que objestos do mesmo cluster são similares entre si e dissimilares dos de outros clusters.

\begin{itemize}
	\item \textbf{Pontos Positivos}: Intuitiva, rápida e boa para uma primeira impressão.
	
	\item \textbf{Pontos Negativos}: Extremamente subjetiva. Torna-se inviável em conjuntos de dados com milhares de observações. Não deve ser usada como único método de validação.
\end{itemize}

A figura \ref{fig:matriz-similaridade-agrupadas} mostra um exemplo de Matriz de Similaridades onde a estrutura de cluster esta bem definida.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\linewidth]{"Cursos/02 - Ciência de Dados, Aprendizado de Máquina e Mineração de Dados/images/Matriz-Similaridades-Agrupadas.png"}
	\caption{Matriz de Similaridades com clusters agrupados}
	\label{fig:matriz-similaridade-agrupadas}
\end{figure}

Ja a figura \ref{fig:matriz-similaridade-dispersas}, observe que dados sem uma estrutura de cluster bem definida se destacam menos na inspeção visual.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\linewidth]{"Cursos/02 - Ciência de Dados, Aprendizado de Máquina e Mineração de Dados/images/Matriz-Similaridades-Dispersas.png"}
	\caption{Matriz de Similaridades com clusters dispersos}
	\label{fig:matriz-similaridade-dispersas}
\end{figure}

\subsection{Exemplo em Python}

\begin{lstlisting}[language=python]
import numpy as np
import matplotlib.pyplot as plt
from scipy.spatial.distance import pdist, squareform
from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans

# Gerar dados sinteticos
X, y = make_blobs(n_samples=100, centers=3, random_state=42)

# Calcular matriz de distancias
dist_matrix = squareform(pdist(X))

# Fazer clustering
kmeans = KMeans(n_clusters=3, random_state=42).fit(X)
labels = kmeans.labels_

# Ordenar a matriz de distancias pelos labels
sort_idx = np.argsort(labels)
sorted_matrix = dist_matrix[sort_idx][:, sort_idx]

# Plotar
plt.figure(figsize=(10, 8))
plt.imshow(sorted_matrix, cmap='viridis', interpolation='nearest')
plt.colorbar()
plt.title('Matriz de Dissimilaridades Ordenada por Cluster')
plt.show()
\end{lstlisting}

\section{Índices de Validade Interna}

Avaliam a qualidade usando apenas os próprios dados e a atribuição de clusters, sem informações extermas.

\subsection{Erro Quadrático (SSE - Sum of Squared Errors)}

É a função custo do algoritimo K-Means . Mede a coesão interna (quão compactos são os clusters).

\begin{itemize}
	\item \textbf{Fórmula}:
	
	\begin{itemize}
			\item $SSE = \sum_{i=1}^{k} \sum_{\mathbf{x} \in C_i} ||\mathbf{x} - \mathbf{\mu}_i||^2 $
			
			\item $k$: Número de clusters.
			
			\item $C_i$: Conjunto de pontos noncluster $i$.
			
			\item $\mu_i$: Centróide (vetor de médias) do cluster $i$.
	\end{itemize}
	
	
	\item \textbf{Para que serve}: Escolher a melhor execução de um algoritmo após várias inicializações (evitando um resultado ruim por azar na escolha inicial dos centróides). É base para o Método do Cotovelo (Elbow Method) para encontrar o número ideal de clusters $k$.
	
	\item \textbf{Interpretação}: Quanto maior o valor, mais compactos e coesos são os clusters.
	
	\item \textbf{Pontos Negativos}: Sempre diminui à medida que $k$ aumenta. Se $k=n$, $SSE = 0$. Portant, não pode ser usado sozinho para escolher $k$.
	
	\item \textbf{Interpretação}: Um SSE de 150.4 indica que a distância total dos pontos para seus centróides é menor que um SSE de 320.7, sugerindo clusters potencialmente mais compactos no primeiro caso.
	
	\item \textbf{Exemplo em Python (Elbow Method)}:
	
	\begin{lstlisting}[language=python]
	from sklearn.cluster import KMeans
	from sklearn.datasets import make_blobs
	
	# Gerar dados
	X, y = make_blobs(n_samples=100, centers=3, random_state=42)
	
	# Executar K-Means
	kmeans = KMeans(n_clusters=3, random_state=42, n_init='auto').fit(X)
	
	# Obter o SSE (chamado de 'inertia_' no scikit-learn)
	sse = kmeans.inertia_
	print(f"SSE para k=3: {sse:.4f}")
	# Output: SSE para k=3: ~300.0 (valor variavel)
	\end{lstlisting}
\end{itemize}


\subsection{Correlação Cofenética}

Avalia a qualidade de um agrupamento hierárquico (ex: ligação simples, completa, média). Mede quão bem o dendrograma preserva as distâncias originais entre os pares de pontos.

\begin{itemize}
	\item \textbf{Para que server}: Validar se o dendrograma gerado é uma representação fiel das distâncias originais entre os dados.
	
	\item \textbf{Como funciona}:
	
	\begin{enumerate}
		\item Calcula a Matriz de Distâncias Originais (ex: Euclidiana) entre todos os pares de pontos.
		
		\item Gera a Matriz Cofenética a partir do dendrograma. A distância cofenética entre dois pontos é a altura no dendrograma onde eles são unidos pela primeira vez.
		
		\item Calcula a Correlação de Pearson entre os valores das duas matrizes (considerando apenas a triangular inferior ou superior, pois são simétricas).
	\end{enumerate}
	
	\item \textbf{Pontos Positivos}: Única métrica interna robusta projetada especificamente para avaliar a qualidade de dendrogramas.
	
	\item \textbf{Pontos Negativos}: Aplicável apenas para agrupamentos hierárquicos. Cálculo da matriz de distâncias é $O(n_2)$, tornando-se inviável para conjuntos de dados muito grandes.
		
	\item \textbf{Interpretação}: O valor da correlação varia entre -1 e 1. Quanto mais próximo de 1, melhor a qualidade do dendrograma e do agrupamento hierárquicos.
	
	\item \textbf{Exemplo em Python}:
	
	\begin{lstlisting}[language=python]
	from scipy.cluster.hierarchy import linkage, cophenet
	from scipy.spatial.distance import pdist
	import numpy as np
	
	# Gerar os mesmos dados
	X, y = make_blobs(n_samples=100, centers=3, random_state=42)
	
	# Calcular a matriz de distancias condensada
	distancias = pdist(X)
	
	# Realizar agrupamento hierarquico
	Z = linkage(distancias, method='average')
	
	# Calcular a correlação cofenetica
	c, coph_dists = cophenet(Z, distancias)
	print(f"Correlação Cofenética: {c:.4f}")
	# Output: Correlação Cofenética: ~0.87 (valor variável)
	\end{lstlisting}
\end{itemize}


\section{Índices de Validade Relativa}

Usados para comparar diferentes agrupamentos (ex: diferentes algoritmos ou diferentes valores de $k$).


\subsection{Índice de Silhueta (Silhouette Score)}

É o índice mais popular e intuitivo. Ele calcula, para cada ponto, o quão bem ele está em seu próprio cluster comparado a outros clusters.

\begin{itemize}
	\item \textbf{Para que serve}: Escolher o número ideal de clusters $k$ e comparar diferentes algoritmos.
	
	\textbf{Fórmula}: Para cada amostra $i$:
	
	\begin{enumerate}
		\item \textbf{Calcule $a(i)$}: Distância média entre $i$ e todos os outros pontos no mesmo cluster (Coesão).
		
		\item \textbf{Calcule $b(i)$}: Distância média entre $i$ e todos os pontos no cluster mais próximo do qual $i$ faz parte (Separação).
		
		\item Calcule o score da silhueta para a amostra $i$
		
		\begin{center}
			$s(i) = \frac{b(i) - a(i)}{max{a(i), b(i)}}$
		\end{center}
		
	\end{enumerate}
	
	\item \textbf{Interpretação}:
	
	\begin{itemize}
		\item $s(i) \approx 1$: O ponto está bem posicionado em seu cluster.
		
		\item $s(i) \approx 0$:  O ponto está muito próximo da fronteira entre dois clusters.
		
		\item $s(i) \approx -1$: O ponto provavelmente foi atribuído ao cluster errado.
		
		\item Um score de silhueta médio de 0.65 para k=3 sugere uma estrutura de clusters boa e muito melhor do que um score de 0.12 para k=2, indicando que k=3 é a melhor escolha.
	\end{itemize}
	
	\item \textbf{Exemplo em Python}:
	
	\begin{lstlisting}[language=python]
	from sklearn.metrics import silhouette_score
	
	# Usando os labels do K-Means anterior
	silhouette_avg = silhouette_score(X, kmeans.labels_)
	print(f"Silhouette Score para k=3: {silhouette_avg:.4f}")
	# Output: Silhouette Score para k=3: ~0.55 (valor variável)
	
	# E comum calcular para vários k
	for k in [2, 3, 4, 5]:
		kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')
		kneans.fit(X)
		score = silhouette_score(X, kmeans.labels_)
		print(f"k = {k}: Silhouette Score = {score:.4f}")
	\end{lstlisting}
\end{itemize}


\section{Índices de Validação Externa}

Comparam a partição encontrada pelo algoritmo com uma partição de referencia ("ground truth") que é considerada correta.

\subsection{Índice RAND (RAND Index)}

Mede a similaridade entre duas partições de dados: a encontrada pelo algoritmo ($U$) e uma partição de referência ($V$). Calcula a proporção de pares de pontos que forma concordantemente agrupados em $U$ e $V$.

\begin{itemize}
	\item \textbf{Para que server}: Validar os resultados de um algoritmo de clustering quando se possui labels verdadeiros. Comum em pesquisa para comparar algoritmos usando bases de dados conhecidas.
	
	\item \textbf{Formula}:
	
	\begin{center}
		$[ RI = \frac{a + b}{a + b + c + d} = \frac{a + b}{\binom{n}{2}} ]$
	\end{center}
	
	\begin{itemize}
		\item $a$: Pares no mesmo cluster em $U$ e no mesmo cluster em $V$.
		
		\item $b$: Pares em clusters diferentes em $U$ e em clusters diferentes em $V$.
		
		\item $c$: Pares no mesmo cluster em $U$ mas em clusters diferentes em $V$.
		
		\item $d$: Pares em clusters diferentes em $U$ mas no mesmo cluster em $V$.
		
		\item $n$: Número total de pontos.
	\end{itemize}
	
	\item \textbf{Pontos Positivos}: Fácil de interpretar. Leva em conta todos os pares de pontos.
	
	\item \textbf{Pontos Negativos}: Crucialmente, requer uma rotulação prévia, o que raramente está disponível em problemas reais de clusterint puro. O valor esperado para duas partições aleatórias não é zero, o que dificulta a interpretação (essa limitação é corrigida pelo Adjusted Rand Index).
	
	\item \textbf{Interpretação}: Varia de 0 a 1. Quanto mais próximo de 1, maior a concordância entre o clustering e a partição de referência. Se o RI for 0.88, significa que 88\% dos pares de pontos tiveram o mesmo destino (ficarem juntos ou separados) tanto no clustering quanto na classificação verdadeira.
	
	\item \textbf{Exemplo em Python}:
	
	\begin{lstlisting}[language=python]
	from sklearn.metrics import rand_score
	
	# Usando os labels verdadeiros (y) e os labels do K-Means (kmeans.labels_)
	ri = rand_score(y, kmeans.labels_)
	print(f"RAND Index: {ri:.4f}")
	# Output: RAND Index: ~0.95 (valor variável, mas alto pois os dados são bem separados)
	\end{lstlisting}
	
\end{itemize}

\section{Exemplos}

\subsection{Exemplo Descritivo: Validando Segmentos de Clientes}

Uma empresa rodou um K-Means em seus dados de clientes e obteve 3 clusters. Para validar, o analista:

\begin{enumerate}
	\item \textbf{Validação Interna/Relativa}: Ele calcula o Score de Silhueta médio para o resultado com $k=3$ e obtém 0.85. Ele também testa com $k=2$ (score 0.70) e $k=4$ (score 0.65). Ele conclui que $k=3$ é a melhor escolha, pois maximizou a silhueta.
	
	\item \textbf{Validação Externa (se possível)}: A empresa tinha uma classificação manual antiga para 100 de seus clientes VIPs. O analista usa essa classificação como "gabarito" e calcula o RAND Index entre o seu agrupamento e o gabarito. Ele obtém um score de 0.92, indicando que os clusters encontrados pelo algoritmo correspondem muito bem à segmentação manual, validando a relevância do modelo para o negócio.
\end{enumerate}


\subsection{Exemplo em Python}

Este código completo demonstra como aplicar os índices WCSS (Cotovelo), Silhueta e RAND para validar um agrupamento.

\begin{lstlisting}[language=python]
# --- Passo 0: Importar as bibliotecas necessarias ---
import matplotlib.pyplot as plt
import numpy as np
from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, rand_score
from scipy.cluster.hierarchy import dendrogram, linkage, cophenet
from scipy.spatial.distance import pdist

# --- Passo 1: Gerar Dados Sinteticos ---
# Criar dados com 4 centros bem definidos. O objetivo e que a validacao aponte k=4.
# X sao as coordenadas, y_true sao os rotulos verdadeiros (nosso "gabarito")
X, y_true = make_blobs(n_samples=500, centers=4, cluster_std=0.8, random_state=42)

# --- Passo 2: Validar K-Means com Indices Internos e Relativos ---
k_range = range(2, 11)
wcss = []
silhouette_scores = []

for k in k_range:
	kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
	kmeans.fit(X)
	wcss.append(kmeans.inertia_)
	silhouette_scores.append(silhouette_score(X, kmeans.labels_))

# --- Plotar os graficos de validacaoo para K-Means ---
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
fig.suptitle('Validacao de K-Means para Encontrar o Numero Ideal de Clusters (k)', fontsize=16)

# Subplot 1: Metodo do Cotovelo (WCSS)
ax1.plot(k_range, wcss, 'bo-')
ax1.set_xlabel('Numero de Clusters (k)')
ax1.set_ylabel('WCSS (Erro Quadratico)')
ax1.set_title('Metodo do Cotovelo')
ax1.grid(True)

# Subplot 2: Score de Silhueta
ax2.plot(k_range, silhouette_scores, 'ro-')
ax2.set_xlabel('Numero de Clusters (k)')
ax2.set_ylabel('Score de Silhueta Medio')
ax2.set_title('Analise de Silhueta')
ax2.grid(True)
plt.show()

# --- Conclusao para K-Means ---
k_ideal_silhueta = k_range[np.argmax(silhouette_scores)]
print(f"O pico no grafico de Silhueta foi em k = {k_ideal_silhueta}, com um score de {max(silhouette_scores):.2f}.")
print("O 'cotovelo' no grafico WCSS tambem sugere k=4.")
print("-" * 50)

# --- Passo 3: Validar com Indice Externo (RAND Index) ---
# Usar o k ideal encontrado (k=4) e comparar com o gabarito (y_true)
kmeans_final = KMeans(n_clusters=k_ideal_silhueta, random_state=42, n_init=10)
kmeans_final.fit(X)
rand_index = rand_score(y_true, kmeans_final.labels_)
print(f"Usando k={k_ideal_silhueta}, o RAND Index em comparacao com o gabarito e: {rand_index:.3f}")
print("Um valor proximo de 1.0 indica uma excelente correspondencia com os clusters verdadeiros.")
print("-" * 50)


# --- Passo 4: Validar Agrupamento Hierarquico com Correlacao Cofenetica ---
print("Avaliando Agrupamento Hierarquico com Correlacao Cofenetica...")
# Calcular o linkage (estrutura do dendrograma)
linked = linkage(X, method='ward')

# Calcular a correlacao cofenetica
# pdist calcula a matriz de distancia original
coph_corr, coph_dist = cophenet(linked, pdist(X))
print(f"A Correlacao Cofenetica do dendrograma e: {coph_corr:.3f}")
print("Um valor proximo de 1.0 indica que o dendrograma preserva bem as distancias originais.")

# Plotar o dendrograma para visualizacao
plt.figure(figsize=(12, 7))
plt.title('Dendrograma do Agrupamento Hierarquico')
dendrogram(linked, truncate_mode='lastp', p=12, leaf_rotation=45., show_contracted=True)
plt.xlabel('Tamanho do Cluster')
plt.ylabel('Distancia (Ward)')
plt.grid(axis='y', linestyle='--')
plt.show()

\end{lstlisting}


\section{Pontos Positivos e Negativos}

\subsection{Pontos Positivos}

\begin{itemize}
	\item \textbf{Objetividade}: Substitui a análise puramente visual por métricas quantitativas, reduzindo a subjetividade.
	
	\item \textbf{Confiança}: Aumenta a confiança de que a estrutura encontrada no agrupamento é significativa e não aleatória.
	
	\item \textbf{Orientação}: Fornece uma base sólida para decisões importantes, como a definição do número de clusters.
\end{itemize}

\subsection{Pontos Negativos}

\begin{itemize}
	\item \textbf{Custo Computacional}: Rodar o algoritmo de agrupamento múltiplas vezes para diferentes valores de $k$ pode ser demorado.
	
	\item \textbf{Ambiguidade}: Diferentes índices podem, às vezes, sugerir diferentes números ideais de $k$, exigindo uma análise mais aprofundada do contexto do problema.
	
	\item \textbf{Não Garante Relevância de Negócio}: Um agrupamento matematicamente "perfeito" pode não ser útil ou acionável do ponto de vista do negócio. A validação quantitativa deve sempre ser complementada por uma análise qualitativa dos clusters encontrados.
\end{itemize}