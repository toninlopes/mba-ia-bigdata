\chapter{Transfer Learning em Redes Neurais Convolucionais}

Tradicionalmente, para treinar uma Rede Neural Convolucional (CNN) do zero, precisaríamos de centenas de milhares de imagens e semanas de processamento em computadores potentes.

O \textbf{Transfer Learning} muda esse paradigma: em vez de começar do zero, nós pegamos uma rede que já foi treinada em um conjunto de dados gigantesco (como o \textbf{ImageNet}, que contém mais de 1 milhão de imagens e 1.000 categorias) e a "reutilizamos" para uma nova tarefa.

A lógica é que as primeiras camadas de uma CNN aprendem características universais, como:

\begin{itemize}
	\item Bordas e linhas;
	\item Curvas e texturas;
	\item Formas geométricas básicas.
\end{itemize}

Essas características são úteis tanto para identificar um carro quanto para identificar uma célula em um microscópio.

\noindent\textbf{Como isso funciona na prática?}

Quando aplicamos Transfer Learning, geralmente seguimos estes passos:

\begin{enumerate}
	\item \textbf{Escolhemos um modelo pré-treinado} (como ResNet, VGG ou MobileNet);
	\item \textbf{Congelamos as camadas iniciais:} Mantemos os "pesos" que a rede já aprendeu, pois eles já sabem detectar formas básicas;
	\item \textbf{Substituímos a "cabeça" de classificação:} Removemos a última camada (que classificava as 1.000 categorias originais) e adicionamos novas camadas para o nosso problema específico (ex: "Cão" vs "Gato").
\end{enumerate}


\section{Fine-tuning}

O \textbf{Fine-tuning} (Ajuste Fino) é o processo de pegar um modelo que já aprendeu a "enxergar" o mundo e dar a ele um "treinamento de especialização".

Imagine que temos uma \textbf{ResNet} que foi treinada no ImageNet. Ela já sabe o que é uma orelha, uma textura de pelo ou o formato de uma pata. Se quisermos que ela identifique raças específicas de gatos domésticos, não precisamos ensiná-la a ver orelhas novamente; só precisamos ajustar as camadas finais para que ela entenda as sutilezas que diferenciam um Siamês de um Persa.

Para entendermos como esse processo é feito tecnicamente, iremos dividir em três etapas principais:

\begin{enumerate}
	\item \textbf{A Preparação da Arquitetura}: Como modificar a rede original;
	\item \textbf{O Congelamento (Freezing)}: Decidir quais partes da rede não devem mudar;
	\item \textbf{A Taxa de Aprendizado (Learning Rate)}: Por que usamos passos muito pequenos nesta fase.
\end{enumerate}


\subsection{A Preparação da Arquitetura}

O primeiro passo é remover a camada de saída original (que tinha 1.000 classes no ImageNet) e substituí-la por uma nova camada que corresponda ao nosso número de classes.

Para começarmos, pense no seguinte: as camadas mais próximas da imagem (no início da rede) detectam linhas e cores. As camadas mais próximas da saída detectam objetos complexos.

Se o seu novo conjunto de dados (ex: fotos de satélite) for muito diferente do original (ex: fotos de animais do ImageNet), você acha que deveríamos manter ou treinar novamente as camadas iniciais da rede?


\subsection{Congelamento (Freezing)}

Essa é uma das partes mais estratégicas do processo. O \textbf{Congelamento (Freezing)} consiste em "trancar" os pesos de certas camadas para que eles não sejam alterados durante o novo treinamento.

Como essas camadas já aprenderam a detectar padrões universais (bordas, texturas, formas) em um banco de dados gigante, não queremos que elas "esqueçam" isso ou que seus pesos sejam bagunçados pelo nosso conjunto de dados menor.

\noindent\textbf{Como funciona o congelamento?}

Geralmente, seguimos uma lógica de "baixo para cima":

\begin{enumerate}
	\item \textbf{Camadas Iniciais (Base):} São quase sempre congeladas. Elas funcionam como os "olhos" da rede, capturando o básico;
	\item \textbf{Camadas Intermediárias:} Podem ser congeladas ou descongeladas, dependendo de quantos dados você tem;
	\item \textbf{Camadas Finais (Cabeça):} São sempre descongeladas (ou novas), pois precisam aprender as características específicas das suas novas classes.
\end{enumerate}

\noindent\textbf{A Regra de Ouro do Congelamento}

A decisão de quanto da rede "congelar" ou "descongelar" depende do equilíbrio entre dois fatores: \textbf{Tamanho do seu Dataset} e \textbf{Similaridade dos Dados}.

\begin{table}[h]
	\begin{tabular}{p{0.48\textwidth} p{0.48\textwidth} }
		Cenário & Estratégia Recomendada \\
		\hline
		\\
		\textbf{Dataset pequeno + Dados similares} (ex: detectar raças de cães) & Congelar quase tudo. Treinar apenas a nova camada de saída.\\
		\\
		\textbf{Dataset grande + Dados diferentes} (ex: fotos de satélite vs. ImageNet) & Descongelar mais camadas (ou toda a rede) para que ela se adapte ao novo domínio.\\
		\\
	\end{tabular}   
\end{table}

\noindent\textbf{Vamos aplicar o raciocínio:}

Imagine que você quer criar um modelo para identificar \textbf{tipos de flores}, mas você tem apenas \textbf{100 fotos} de cada flor. Você está usando uma rede pré-treinada no ImageNet (que já continha algumas flores e muitos objetos naturais).

Nesse caso de um \textbf{dataset pequeno} e \textbf{dados razoavelmente similares} ao original, por que seria arriscado descongelar a rede inteira para treinar tudo do zero?


\subsection{A Taxa de Aprendizado (Learning Rate)}

A \textbf{Taxa de Aprendizado} é como o "tamanho do passo" que o algoritmo dá enquanto busca o ponto de menor erro. No contexto de \textbf{Fine-tuning}, essa é uma das configurações mais críticas para o sucesso do modelo.

\noindent\textbf{Por que usar uma taxa bem pequena?}

Quando fazemos o ajuste fino, partimos de uma rede que já está "educada". Se usarmos uma taxa de aprendizado alta (passos largos), corremos o risco de:

\begin{itemize}
	\item \textbf{Destruir o Conhecimento Prévio:} Pesos que já eram bons para detectar formas e texturas seriam alterados de forma brusca, um fenômeno chamado de esquecimento catastrófico;
	\item \textbf{Instabilidade:} O modelo pode "pular" o ponto ideal de ajuste e nunca convergir para uma boa solução no seu novo conjunto de dados.
\end{itemize}

Geralmente, no Fine-tuning, usamos uma taxa que é \textbf{10 a 100 vezes menor} do que a usada no treinamento original da rede.


\section{Classificadores}

Depois de termos explorado como ajustar os pesos da nossa rede (o fine-tuning), o próximo passo natural é decidir quem será o "tomador de decisão" final.

Quando usamos \textbf{Transfer Learning}, a rede convolucional (CNN) funciona como um extrator de características complexas. O que colocamos no topo é o classificador que interpretará esses dados para dar a resposta final.

Vamos explorar três opções de classificadores:

\begin{itemize}
	\item \textbf{Camadas Totalmente Conectadas (MLP) com Softmax:}
	\subitem É a escolha clássica. Pegamos os dados da CNN, "achatamos" (Flatten) e passamos por camadas densas. A função \textbf{Softmax} no final transforma os números em probabilidades (ex: 90\% de chance de ser um gato).
	
	\item \textbf{Global Average Pooling (GAP):}
	\subitem Em vez de usar camadas densas gigantescas, o GAP tira a média de cada mapa de características. Isso reduz drasticamente o número de parâmetros da rede, o que ajuda a evitar o overfitting e torna o modelo mais leve.
	
	\item \textbf{Classificadores Externos (como SVM):}
	\subitem Aqui, usamos a CNN apenas para extrair um vetor de números (características) e passamos esses dados para um algoritmo de Machine Learning tradicional, como a \textbf{Máquina de Vetores de Suporte (SVM)}, que tentará encontrar a melhor fronteira de separação entre as classes.
\end{itemize}


\section{Resumo}

O \textbf{Transfer Learning} é uma técnica fundamental no aprendizado profundo que consiste em \textbf{reutilizar modelos previamente treinados} para resolver novos problemas, reduzindo significativamente o tempo de treinamento e a necessidade de grandes volumes de dados. Em \textbf{Redes Neurais Convolucionais (CNNs)}, essa abordagem é especialmente eficaz devido à natureza hierárquica das representações aprendidas.

O treinamento de CNNs profundas a partir do zero exige alto custo computacional e grandes bases de dados, podendo levar dias ou semanas. Em contrapartida, modelos pré-treinados em grandes conjuntos de dados, como o \textbf{ImageNet}, aprendem representações genéricas que podem ser aproveitadas em diferentes tarefas de visão computacional. Esses modelos capturam, nas camadas iniciais, padrões básicos como bordas, texturas e formas simples, enquanto camadas mais profundas aprendem características mais específicas.

Uma CNN pode ser dividida em duas partes principais: a \textbf{base convolucional}, responsável pela extração de atributos, e o \textbf{classificador}, responsável pela tomada de decisão final. No Transfer Learning, normalmente a base convolucional do modelo pré-treinado é reutilizada, enquanto o classificador original é removido e substituído por um novo classificador adequado ao problema-alvo.

**As três estratégias principais de uso do Transfer Learning** são:

\begin{enumerate}
	\item \textbf{Treinar todo o modelo}: todos os pesos do modelo pré-treinado são ajustados com os novos dados. Essa abordagem é indicada quando há um conjunto de dados grande e semelhante ao domínio original, mas exige maior poder computacional e cuidado com overfitting;
	
	\item \textbf{Treinar parcialmente o modelo}: algumas camadas, geralmente as mais profundas, são ajustadas, enquanto as camadas iniciais permanecem congeladas. Essa estratégia equilibra adaptação ao novo problema e reaproveitamento do conhecimento prévio;
	
	\item \textbf{Congelar a base convolucional}: a base é utilizada como um extrator fixo de atributos, e apenas o novo classificador é treinado. É a estratégia mais comum quando o conjunto de dados é pequeno ou quando há limitação de recursos computacionais.
\end{enumerate}

Um ponto crítico é o uso de \textbf{taxas de aprendizado baixas} durante o ajuste fino (fine-tuning), especialmente quando camadas pré-treinadas são atualizadas. Taxas altas podem destruir o conhecimento previamente aprendido, prejudicando o desempenho do modelo.

A escolha da melhor estratégia depende de dois fatores principais: o \textbf{tamanho do conjunto de dados} e a \textbf{similaridade entre o problema-alvo e o problema original} do modelo pré-treinado. De forma geral, quanto maior o dataset e maior a similaridade entre os domínios, maior a viabilidade de ajustar mais camadas da rede.

Por fim, diferentes opções de \textbf{classificadores} podem ser utilizados após a base convolucional, como camadas totalmente conectadas com softmax, \textbf{Global Average Pooling}, que reduz o número de parâmetros, ou classificadores externos, como \textbf{SVMs}.

Em síntese, o Transfer Learning é uma abordagem prática e eficiente para aplicar CNNs profundas em novos problemas, permitindo obter bons resultados mesmo em cenários com \textbf{poucos dados e recursos computacionais limitados}, sendo amplamente adotado em aplicações modernas de visão computacional.


\subsection{Mapa Mental}

\begin{figure}[h]
	\centering
	\includegraphics[width=1\linewidth]{"Cursos/05 - Redes Neurais e Deep Learning/imagens/MapaMentalCNNTransferLearning.png"}
	\caption{Mapa Mental Transfer Learning em Redes Neurais Convolucionais}
	\label{fig:mapamentalcnntransferlearning}
\end{figure}